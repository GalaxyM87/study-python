# 数据拟合

## 误差介绍

### 1.discrepency and error

discrepency：指的是两次测量之间的差别，因为我们测量具有不确定性，我们不知道我们测到的是不是真实值。

error：指的是测量值和真实值之间的差别

如果我们希望了解误差，那么我们就要对一个物理量进行重复测量，研究它的分布。



### 2.概率密度函数$p(x)$


$$
\int \operatorname{p(x)} d x=1
$$
$p(x) \Delta x$  为变量处在 $x \rightarrow x+\Delta x$ 的概率，$\Delta N = p(x) \Delta x$ 为测量N次，测量量在 $x \rightarrow x+\Delta x$的次数，对x积分既可得到在N次测量，在某一个区间里内可以测到几次。



### 3.parent distribution and sample distribution

***他们都是概率分布函数***

parent distribution：我们在测量某个物理量时，它的真实值为x，我测量得到值x1不一定和x相等，但我测量多次，会有一个分布出现，当我们测量无限次，就可以准确得到数据点的分布，我们可以假设这种分布为parent distribution，它会决定我每次测量时出现某个值的概率。

sample distribution：假设我们每次的测量都是在parent distribution中取样，构成取样分布，当我们测量次数达到无限次后，sample distribution变成了parent distritbution。





### 4.统计的基本量

***parent distrition的统计量用希腊字母表示，experimental(sample) distribution用英文字母表示***

对一个实验我们进行N次观测，每一次观测即为$x_i$，我们做N次测量，可以得到
$$
\sum_{i=1}^{N} x_{i} \equiv x_{1}+x_{2}+x_{3}+\cdots+x_{N}
$$
为了写作方便，我们记为 $\sum x_{i} \equiv \sum_{i=1}^{N} x_{i}$



#### 平均数（Mean）

对于实验数据，
$$
\bar{x}=\frac{1}{N} \sum x_{i}
$$
当我们测量无数次后，我们可以得到parent population的平均值
$$
\mu=\lim _{N \rightarrow \infty}\left(\frac{1}{N} \sum x_{i}\right)
$$

#### 中位数（Median）

在无限次的测量中，半数测量大于$x_i$，半数测量小于$x_i$，$x_i$即为中位数


$$
P\left(x_{i}<\mu_{1 / 2}\right)=P\left(x_{i} \geqslant \mu_{1 / 2}\right)=\frac{1}{2}
$$


#### 众数（Mode）

常被称为mode or most probable value $\mu_{i}$
$$
P\left(\mu_{\max }\right) \geqslant P\left(x \neq \mu_{\max }\right)
$$

### 5.deviation

将测量值$x_{i}$和parent population中的平均值记为deviation，我们简写为$d_{i}$
$$
d_{i} \equiv x_{i}-\mu
$$
如果$\mu$为真实值，则$d_{i}$为真实误差error



为了表示测量值对于平均值的弥散度（dispersion），我们引入方差$\sigma^{2}$（square of the deviation）和标准差（stardard deviation）$\sigma$，
$$
\sigma^{2} \equiv \lim _{N \rightarrow \infty}\left[\frac{1}{N} \sum\left(x_{i}-\mu\right)^{2}\right]=\lim _{N \rightarrow \infty}\left(\frac{1}{N} \sum x_{i}{ }^{2}\right)-\mu^{2}
$$
$\sigma = \sqrt{\sigma^{2}}$，为方差的平方根



### 6.$s^2$

$$
S^{2} \equiv \frac{1}{N-1} \sum\left(x_{i}-\bar{x}\right)^{2}
$$

它表征的是实验的不确定性

### 7.离散分布与连续分布

我们在这里利用parent distribution概率分布函数p(x)来定义平均值 $\mu$ 和 标准差 $\sigma$

***离散分布：概率函数p(x)是观测值x的离散函数***

假设我们的量x有 j 个不同的观测值（从$x_1 \rightarrow x_j$） ，观测N次，那么观测得到各值的次数应该为$NP(x_j)$，此时平均值$\mu$和标准差$\sigma$为
$$
\mu=\lim _{N \rightarrow \infty} \frac{1}{N} \sum x_{j}=\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{j=1}^{n}\left[x_{j} N P\left(x_{j}\right)\right]=\lim _{N \rightarrow \infty } \sum_{j=1}^{n}\left[x_{j} P\left(x_{j}\right)\right]
$$

$$
\sigma^{2}=\lim _{N \rightarrow \infty} \sum_{j=1}^{n}\left[\left(x_{j}-\mu\right)^{2} P\left(x_{j}\right)\right]=\lim _{N \rightarrow \infty} \sum_{j=1}^{n}\left[x_{j}{ }^{2} P\left(x_{j}\right)\right]-\mu^{2}
$$

对于任意一个函数$f(x)$的期望为
$$
\langle f(x) \rangle =\sum_{i=1}^{n}\left[f\left(x_{j}\right) P\left(x_{j}\right)\right]
$$
概率最后应该归1，$\sum_{j=1}^{n} p(x_j) = 1$

***连续分布：概率函数p(x)是观测值x的连续平滑函数***

平均值$\mu$变为parent distribution的一阶矩
$$
\mu=\int_{-\infty}^{\infty} x P(x) \mathrm{d} x
$$


方差$\sigma^{2}$变为了二次中心乘积矩
$$
\sigma^{2}=\int_{-\infty}^{\infty}(x-\mu)^{2} P(x) d x=\int_{-\infty}^{\infty} x^{2} P(x) \mathrm{d} x-\mu^{3}
$$
对于任意函数$f(x)$的期望，
$$
\langle f(x)\rangle=\int_{-\infty}^{\infty} f(x) P(x) \mathrm{d} x
$$

## 三种统计模型

### 1.二项分布

排列数$P_m(x)$：

指的是有多少种排列方式，其中考虑了次序，比如在掷硬币的实验中，我们将向上和向下的结果比拟成两个盒子，投掷n枚硬币，我们比拟成n个小球，我们给小球标号（$1 \rightarrow n$）一开始我们先选定小球，从第一次开始，将小球投入盒子，如果我们希望知道n次投掷中，有x次为向上的排列数，那么，第一次投掷时，我们从n个小球里面选一个，投进盒子，第二次投掷，从n-1里面选一个小球，投进盒子，我们选定以后，次序也固定了，比如我的x=2，我先让序号为3小球投进，再让序号为4的球投进向上的箱子，那么为排列数为$n(n-1)$，以此类推，x次向上的排列数为，
$$
P_{m}(n, x)=n(n-1)(n-2) \cdots(n-x+2)(n-x+1)=\frac{n !}{(n-x) !}
$$


组合数$C(n,x)$：

我们如果不考虑先后投球的次序，我们只考虑最后有多少个投入向上的盒子，多少个投入向下的盒子，即为组合数，我们可以从直觉上知道，这样的组合数一定比计算次序的排列数要少得多，那么我们就要去掉球的次序对我们产生的影响，比如上面x=2，我先让序号为3的小球投进，再让序号为4的小球投进，和我先让序号为4的小球投进，再让序号为3的小球投进，排列是不一样的，但是在组合里面都为一种情况，所以我们要除掉排列的简并因子$x !$，即为，
$$
C(n, x)=\frac{P_{m}(n, x)}{x !}=\frac{n !}{x !(n-x) !}=\left(\begin{array}{l}
n \\
x
\end{array}\right)
$$


概率$P_{B}(x;n,p)$：

前面我们讨论了排列数和组合数，那么我们要知道投掷n枚硬币，x枚硬币朝上的概率，即为组合数量乘以每种组合对应的概率$\left(\frac{1}{2}\right)^{n}$,它可以分为x枚硬币向上的概率$\left(\frac{1}{2}\right)^{x}$，和$(n-x)$枚硬币向下的概率$\left(\frac{1}{2}\right)^{(n-x)}$。

于是对于更普适的情况，观察到n个事件中，x个事件处于状态P的概率为，
$$
P_{B}(x ; n, p)=\left(\begin{array}{l}
n \\
x
\end{array}\right) p^{x} q^{n-x}=\frac{n !}{x !(n-x) !} p^{x}(1-p)^{n-x}
$$
同样的，最后概率之和应该归一。



平均值和方差：

由前面离散分布推得的公式，我们可以得到二项分布的平均值和方差为，
$$
\mu=\sum_{x=0}^{n}xp_B(x;n,p)=\sum_{x=0}^{n}\left[x \frac{n !}{x !(n-x) !} p^{x}(1-p)^{n-x}\right]=n p
$$

$$
\sigma^{2}=\sum_{x=0}^{n}\left[(x-\mu)^{2} \frac{n !}{x !(n-x) !} p^{x}(1-p)^{n-x}\right]=n p(1-p)
$$

如果$p=q$，则二项分布图像应该关于平均值对称，且平均值 = 中值 = 最可几值，方差等于平均值的一半。

如果$p \neq q$，则二项分布不对称，方差更小

![binomial](D:\vscode_file\binomial.png)



### 2.泊松分布

核心思想：用平均值来替代发生的概率$p$，因为很多情况下我们并不知道概率，我们只知道在一段事件内接收到的事件数，所以我们将时间分成很多段，求平均值，获得泊松分布。



对于$n \rightarrow \infty$的情况下，且$p<<1$，我们可以将二项分布拓展到泊松分布
$$
P_{B}(x ; n, p)=\frac{1}{x !} \frac{n !}{(n-x) !} p^{x}(1-p)^{-x}(1-p)^{n}
$$

$$
\lim _{n \rightarrow \infty}\frac{n !}{(n-x) !}=\lim _{n \rightarrow \infty}n(n-1)(n-2) \cdots(n-x-2)(n-x-1)=n^{x}
$$

第二项和第三项合并为${\left(np\right)}^{x}={\mu}^{x}$，第四项近似等于1，第五项
$$
\lim _{p \rightarrow 0}(1-p)^{n}=\lim _{p \rightarrow 0}\left[(1-p)^{1 / p}\right]^{\mu}=\left(\frac{1}{\mathrm{e}}\right)^{\mu}=\mathrm{e}^{-\mu}
$$
这里利用了无穷小替换公式$ln(1+x) \sim x \rightarrow ln(1-x) \sim -x$

最后我们可以得到，公式变为了泊松公式
$$
\lim _{p \rightarrow 0} P_{B}(x ; n, p)=P_{p}(x ; \mu) \equiv \frac{\mu^{x}}{x !} \mathrm{e}^{-\mu}
$$
同样的，我们可以得到平均值$\mu$和方差$\sigma^{2}$，只和参数$\mu$有关
$$
\langle x\rangle=\sum_{x=0}^{\infty}\left(x \frac{\mu^{x}}{x !} \mathrm{e}^{-\mu}\right)=\mu \mathrm{e}^{-\mu} \sum_{x=1}^{\infty} \frac{\mu^{x-1}}{(x-1) !}=\mu \mathrm{e}^{-\mu} \sum_{y=0}^{\infty} \frac{\mu^{y}}{y !}=\mu
$$

$$
\sigma^{2}=<(x-\mu)^{2}>=\sum_{x=0}^{\infty}\left[(x-\mu)^{2} \frac{\mu^{x}}{x !} \mathrm{e}^{-\mu}\right]=\mu
$$

总和概率：

假设我们知道泊松分布的平均值$\mu$，我们需要重视一点，泊松分布仍旧为离散分布，它也只当变量$x$为整数时被定义。我们想知道测量数据在$x_{1} \rightarrow x_{2}$的概率，我们需要求和$x_{1}$和$x_{2}$之间所有的整数。
$$
S_{p}\left(x_{1}, x_{2} ; \mu\right)=\sum_{x_{1}}^{x_{2}} P_{p}(x ; \mu)
$$
如果我们想知道给定范围内测量到n次或者更多的概率，
$$
S_{p}(n, \infty ; \mu)=\sum_{x=n}^{\infty} P_{p}(x ; \mu)=1-\sum_{x=0}^{n-1} P_{p}(x ; \mu)=1-\mathrm{e}^{-\mu \sum_{x=0}^{n-1} \frac{\mu^{x}}{x !}}
$$
### 3.高斯分布

高斯分布的概率密度函数是连续的，$\mu$和$\sigma$分别为平均值和标准差
$$
P_{G}=\frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right]
$$
因为高斯分布为连续函数，和二项分布，泊松分布的离散分布不同，我们必须要规定测量落在某个区间的概率，在图上可以看作是面积。
$$
\mathrm{d} P_{G}(x ; \mu, \sigma)=P_{G}(x ; \mu, \sigma) \mathrm{d} x
$$
因为概率密度要归一，因为是连续函数，需要积分，
$$
\int_{x=-\infty}^{x=\infty} \mathrm{d} P_{G}(x ; \mu, \sigma)=\int_{x=-\infty}^{x-\infty} P_{G}(x ; \mu, \sigma) \mathrm{d} x=1
$$
我们由概率密度函数可以定义半高全宽$\Gamma$，半高意思为最大值的一半，全宽为曲线的整个宽度，
$$
P_{G}\left(\mu \pm \frac{\Gamma}{2} ; \mu, \sigma\right)=\frac{1}{2} P_{G}(\mu ; \mu, \sigma)
$$
***积分概率：我们如果想知道偏离平均值小于$\Delta x$的概率，我们要对概率密度函数积分***
$$
P_{G}(\Delta x ; \mu, \sigma)=\frac{1}{\sigma \sqrt{2 \pi}} \int_{\mu-\Delta x}^{\mu+\Delta x} \exp \left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right] \mathrm{d} x
$$
我们一般利用几个$\sigma$的偏差衡量，比如测量偏离平均值1个$\sigma$内的概率由上式可以计算得到为68%，偏离2个$\sigma$的概率为95%



## 误差分析

### 1.仪器误差和统计误差



对于计数实验，满足我们前面讨论的泊松分布，但因为我们很难多次测量x，但是泊松分布的平均值和方差满足，
$$
\mu=\sigma^{2}
$$
相对误差为，
$$
\frac{\sigma}{\mu}=\frac{1}{\sqrt{\mu}}
$$
对于单次测量，我们可以用$\sqrt{x}$来代替标准差，x代替均值，所以相对误差为，
$$
\frac{\sigma_{x}}{x}=\frac{1}{\sqrt{x}}
$$
可以看出，如果每次实验的计数率x增加，相对误差会减小，增加间隔时间就可以很好的减少相对误差。例如1s中实验计数为1000次，我将时间延长为4s，那么相对误差对应的就会减小到之前的二分之一。

### 2.误差传播

如果我们想测量的x为因变量，它至少和两个可测变量$u,v$有函数关系，
$$
x=f(u,v,\cdots)
$$
我们可以假定最可几值由下式给出，
$$
\bar{x}=f(\bar{u},\bar{v},\cdots)
$$
我们可以知道，在无数次测量的情况下，取样分布的平均值和上面的$\bar{x}$相同，方差为，
$$
\sigma_{x}^{2} \equiv \lim _{N \rightarrow \infty}\left[\frac{1}{N} \sum\left(x_{i}-\bar{x}\right)^{2}\right]
$$
根据多元函数的泰勒展开，我们只取第一项，可以用$u,v$的deviation来代替$x$的deviation。每个偏导数都是在别的变量平均值上计算得到。
$$
x_{i}-\bar{x} \approx\left(u_{i}-\bar{u}\right)\left(\frac{\partial x}{\partial u}\right)+\left(v_{i}-\bar{v}\right)\left(\frac{\partial x}{\partial v}\right)+\cdots
$$
***方差和协方差***

我们利用上面的公式，将方差$\sigma_{x}^{2}$展开为$u,v$的函数，
$$
\begin{aligned}
&\sigma_{x}{ }^{2} \approx \lim _{N \rightarrow \infty} \frac{1}{N} \sum\left[\left(u_{i}-\bar{u}\right)\left(\frac{\partial x}{\partial u}\right)+\left(v_{i}-\bar{v}\right)\left(\frac{\partial x}{\partial v}\right)+\cdots\right]^{2} \\
&\approx \lim _{N \rightarrow \infty} \frac{1}{N} \sum\left[\left(u_{i}-\bar{u}\right)^{2}\left(\frac{\partial x}{\partial u}\right)^{2}+\left(v_{i}-\bar{v}\right)^{2}\left(\frac{\partial x}{\partial v}\right)^{2}+2\left(u_{i}-\bar{u}\right)\left(v_{i}-\bar{v}\right)\left(\frac{\partial x}{\partial u}\right)\left(\frac{\partial x}{\partial v}\right)+\cdots\right]
\end{aligned}
$$
式子的前两项为变量$u,v$的方差$\sigma_{u}^{2}$，$\sigma_{v}^{2}$
$$
\sigma_{u}^{2} \equiv \lim _{N \rightarrow \infty}\left[\frac{1}{N} \sum\left(u_{i}-\bar{u}\right)^{2}\right]\\
\sigma_{v}^{2} \equiv \lim _{N \rightarrow \infty}\left[\frac{1}{N} \sum\left(v_{i}-\bar{v}\right)^{2}\right]
$$
第三项为协方差$\sigma_{uv}$
$$
\sigma_{uv}^{2} \equiv \lim _{N \rightarrow \infty}\left[\frac{1}{N} \sum\left(u_{i}-\bar{u}\right) \left(v_{i}-\bar{v}\right)\right]
$$
我们可以得到形式简洁的误差传播表达式，
$$
\sigma_{x}{ }^{2} \approx \sigma_{u}{ }^{2}\left(\frac{\partial x}{\partial u}\right)^{2}+\sigma_{v}{ }^{2}\left(\frac{\partial x}{\partial v}\right)^{2}+\cdots+2 \sigma_{u v}{ }^{2}\left(\frac{\partial x}{\partial u}\right)\left(\frac{\partial x}{\partial v}\right)+\cdots
$$
如果$u,v$的涨落并不相关，在大量的随机观测下协方差这一项会消失，只剩下,
$$
\sigma_{x}{ }^{2} \approx \sigma_{u}{ }^{2}\left(\frac{\partial x}{\partial u}\right)^{2}+\sigma_{v}{ }^{2}\left(\frac{\partial x}{\partial v}\right)^{2}+\cdots
$$

### 3.具体实例的误差公式

上面我们假设因变量$x$和测量量$u,v$存在函数关系，在这里我们举一些简单的例子，

$(1)$简单和差$(Simple\ Sums\ and\ Differences)$：因变量$x=u+a$
$$
\frac{\partial x}{\partial u}=1
$$
从而$\sigma_{x}=\sigma_{u}$，相对误差为，
$$
\frac{\sigma_{x}}{x}=\frac{\sigma_{u}}{x}=\frac{\sigma_{u}}{u+a}
$$
如果$u$和$a$相差较小，可能会导致$x$的误差很大。

例如：在记录放射源衰变放出粒子的实验中，测量到在实验开始的15 s时间间隔内计数 $N1 =723$，在随后的15 s中计数$N2=19$。衰变事件是随机的并遵守泊松统计，因而知道$N1$和$N2$的误差正是它们的平方根。 假定在没有放射源的情况下非常仔细地测量本底计数率，得到在同样的时间间隔$\Delta t$中可以忽略误差得到本底计数$B=14. 2$。因为是在长时间间隔内取平均值，所以在15 s内的本底计数的平均值不是一个整数。

对于第一个时间间隔，经过本底校正后，
$$
x_{1}=N1-B=708.8
$$
利用误差传播公式，
$$
\sigma_{x_{1}}=\sigma_{N_1}=\sqrt{723}\simeq 26.9
$$
于是相对误差为
$$
\frac{\sigma_{x_{1}}}{x_{1}}=\frac{\sigma_{N_{1}}}{x_{1}}=\frac{\sigma_{N_{1}}}{N_{1}-B}
=\frac{26.9}{708.8}\simeq 3.8\%
$$
对于第二个时间间隔，我们会发现
$$
\frac{\sigma_{x_{2}}}{x_{2}}=\frac{\sigma_{N_{2}}}{x_{2}}=\frac{\sigma_{N_{2}}}{N_{2}-B}=\frac{4.8}{4.8}\simeq 92\%
$$
误差非常大。

$(2)$加权和差$(Weighted\ Sums\ and\ Differences)$：因变量$x=av+bv$

我们计算偏导数，得到，
$$
\left(\frac{\partial x}{\partial u}\right)=a\\
\left(\frac{\partial x}{\partial v}\right)=b
$$
所以我们得到误差传播公式，
$$
\sigma_{x}{ }^{2}=a^{2} \sigma_{u}{ }^{2}+b^{2} \sigma_{v}{ }^{2}+2 a b \sigma_{u v}{ }^{2}
$$
例如：上面的计数实验，我们只测量一次本底计数率$B=14$，所以它也是个变量，有误差存在，那么，
$$
x=N-B
$$
我们根据加权和差的公式，
$$
\left(\frac{\partial x}{\partial N}\right)=1
\left(\frac{\partial x}{\partial B}\right)=-1
$$
最终我们得到误差，
$$
\sigma_{x}{ }^{2}=\sigma_{N}{ }^{2}+\left(-\sigma_{B}\right)^{2}=N+B
$$
对于第一次实验，相对误差为，
$$
\frac{\sigma_{x}}{x}=\frac{\sqrt{N+B}}{N-B}=\frac{\sqrt{723+14}}{723-14}=3.8\%
$$
所以，
$$
x_{1}=(723-14)\pm \sqrt{723+14}=709\pm 27.1 (counts)
$$


对于第二次实验，相对误差为，
$$
\frac{\sigma_{x}}{x}=\frac{\sqrt{N+B}}{N-B}=\frac{\sqrt{19+14}}{19-14}=114\%
$$

$$
x_{1}=(19-14)\pm \sqrt{19+14}=5\pm 5.7 (counts)
$$

$(3)$乘除$(Multiplication\ and\ Division )$：因变量$x=auv$

我们首先求偏导为，
$$
\frac{\partial x}{\partial u}=a v\left(\frac{\partial x}{\partial v}\right)=a u
$$
误差传播公式为，
$$
\sigma_{x}{ }^{2}=\left(a v \sigma_{u}\right)^{2}+\left(a u \sigma_{v}\right)^{2}+2 a^{2} u v \sigma_{u v}{ }^{2}
$$
我们两边同除$x^{2}$并且带入$x=auv$，写成一个更简洁的形式，
$$
\frac{\sigma_{x}{ }^{2}}{x^{2}}=\frac{\sigma_{u}{ }^{2}}{u^{2}}+\frac{\sigma_{v}{ }^{2}}{v^{2}}-2 \frac{\sigma_{u v}{ }^{2}}{u v}
$$
如果因变量$x=\frac{au}{v}$，相对误差为，
$$
\frac{\sigma_{x}{ }^{2}}{x^{2}}=\frac{\sigma_{u}{ }^{2}}{u^{2}}+\frac{\sigma_{v}{ }^{2}}{v^{2}}+2 \frac{\sigma_{u v}{ }^{2}}{u v}
$$

## 样本参数和误差

### 1.最大似然法

假如在实验中获得了N个数据点，它们是从parent population随机抽取的，符合parent distribution。

假设parent distribution是高斯分布，参数为平均值$\mu$和标准差$\sigma$。

如果我们要计算单次观测值$x_{i}$落到$dx$范围内的概率，
$$
\mathrm{d} P_{i}=p_{i} \mathrm{~d} x
$$
概率函数$P_{i}$为高斯函数$P_{G}(x_{i};\mu,\sigma)=\frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right]$

为了简化计算，我们由下式表示观察值$x_{i}$出现概率，
$$
P_{i}=\frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2}\right]
$$
因为我们是不知道parent distribution的参数$\mu$和$\sigma$的，我们需要用已知的实验推导最有可能的parent distribution，这时候我们就要使用最大似然法。

我们首先假设有一个实验分布，观测值为$x_{i}$的概率由以下概率函数给出，当然这就是最普遍的情况，
$$
P_{i}\left(\mu^{\prime}\right)=\frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{x_{i}-\mu^{\prime}}{\sigma}\right)^{2}\right]
$$
考虑这N次观测，我们得到了一系列的观测数据$(x_{1} \rightarrow x_{N})$，得到这些数据的概率为，
$$
P\left(\mu^{\prime}\right)=\prod_{i}^{N} P_{i}\left(\mu^{\prime}\right)=\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^{N} \exp \left[-\frac{1}{2} \sum\left(\frac{x_{i}-\mu^{\prime}}{\sigma}\right)^{2}\right]
$$
最大似然法告诉我们有很多种parent distribution可以产生这样的结果，如果我们假设这些parent distribution的$\sigma$相同，只有$\mu$不同，如果有一个$\mu$使得这个概率值最大，这个参数的parent distribution就是对$\mu^{\prime}$最好的估计。

我们令，
$$
X=-\frac{1}{2} \sum\left(\frac{x_{i}-\mu^{\prime}}{\sigma}\right)^{2}
$$
只要使X的导数为0，就可以求得极值，它使得概率$P(\mu^{\prime})$最大，
$$
\frac{\mathrm{d} X}{\mathrm{~d} \mu^{\prime}}=-\frac{1}{2} \sum \frac{\mathrm{d}}{\mathrm{d} \mu^{\prime}}\left(\frac{x_{i}-\mu^{\prime}}{\sigma}\right)^{2}=\sum\left(\frac{x_{i}-\mu^{\prime}}{\sigma^{2}}\right)=0
$$
最终我们得到，
$$
\mu^{\prime}=\bar{x} \equiv \frac{1}{N} \sum x_{i}
$$
这就是通过最大似然法求出来的对于$\mu^{\prime}$的最好估计。

### 2.通过最大似然法求得的误差

我们再用最大似然法求得$\mu^{\prime}$的方差可以用误差传播方差求出，
$$
\sigma_{\mu}{ }^{2}=\sum\left[\sigma_{i}{ }^{2}\left(\frac{\partial \mu^{\prime}}{\partial x_{i}}\right)^{2}\right]
$$
在这里我们忽略了各个点之间的协方差，以及更高次的误差，因为不是一个点主导，所以近似也是合理的。

如果各数据点误差相同，$\sigma_{i}=\sigma$
$$
\begin{aligned}
&\frac{\partial \mu^{\prime}}{\partial x_{i}}=\frac{\partial}{\partial x_{i}}\left(\frac{1}{N} \sum x_{i}\right)=\frac{1}{N} \\
&\sigma_{\mu}{ }^{2}=\sum\left[\sigma_{i}{ }^{2}\left(\frac{1}{N}\right)^{2}\right]=\frac{\sigma^{2}}{N}
\end{aligned}
$$
可以发现，增加测量次数N，***会让平均值的误差减少。***

我们由第一章的公式可以计算parent distribution的标准差$\sigma$，这里是用s来代替$\sigma$
$$
\sigma \approx s=\sqrt{\frac{1}{N-1} \sum\left(x_{i}-\bar{x}\right)^{2}}
$$
$\star$**同样要注意一个问题，从数据本身得到的$\sigma$应该和从实验设备本身得出的$\sigma$相同。**

假设各数据点误差相同，利用前面公式，对于平均值的误差为，
$$
\sigma_{\mu}=\frac{\sigma}{\sqrt{N}} \approx \frac{s}{\sqrt{N}}
$$
$\star$**我们要注意一个很关键的问题，数据的标准差不会随着实验增多而减少，这已经被各种条件确认，但是平均值的误差会随着测量次数减少，数据的矩形图可以表明这种变化，随着次数增多，曲线会越来越光滑，峰值就会越来越窄，误差越来越小。**

![物理科学中的数据处理和误差分析_12407228](C:\Users\Administrator\Desktop\物理科学中的数据处理和误差分析_12407228.bmp)



我们再次来分析一下这次实验，我们实验测了很多次小球下落的时间，我们在第一章假设存在这样一种parent distribution，决定了我们每次实验获得不同数据的概率，我们可以通过我们测量的数据来推测parent distribution，我们通过数据可以求得我的样本误差s，我们用s来近似$\sigma$然后和我的实验误差去对比，二者应该相同。

根据最大似然法我们可以知道，对于平均值的最好估计就是$\bar{x}$，我们用s来近似$\sigma$，$\sigma$是parent distribution的标准差，通过它我们可以来求一下我平均值的误差$\sigma_{\mu}$，由上面文字我们可以了解过程，
$$
\sigma \approx s=\sqrt{\frac{1}{N-1} \sum\left(x_{i}-\bar{x}\right)^{2}}=0.02
$$

$$
\sigma_{\mu}=\frac{\sigma}{\sqrt{N}} \approx \frac{s}{\sqrt{N}} \approx 0.0028
$$

我们现在可以利用高斯分布的特点，
$$
\frac{|T_{exp}-T_{est}|}{\sigma_{\mu}}=1.4
$$
根据高斯函数的积分，测量落在1.4个偏差内的概率为$83.8\%$，预计有$16.2\%$的测量将超过1.4个偏差。



### 3.统计学的局限性

$(1)$时间和资源有限，实验不可能无限次的重复

$(2)$我们不能了解所有的系统误差，修正所有已知的问题

$(3)$非统计性涨落，实验值很少可以超过3，4个$\sigma$，有出格的数据点说明在好数据点中间也可能夹杂着别的数据点，需要对可能的因素细致的分析

经验教训：对于尾端的数据我们要仔细推敲，而不是轻易的相信。

***数据点的舍弃：***

Chauvenet准则：应该舍弃某个点，它满足，预计离开平均值比该可疑数据点更远的**事件概率**小于半次。换句话而言就是它就不应该出现，但是它出现了，我们以0.5次的概率为界，超过0.5次还有可能会出现，但是小于0.5次我们就应当理解为不应出现，这是一个很可能出现错误的点。

如果数据分布满足高斯分布，我们就有把握舍弃这些点，然后重新计算平均值和标准差，但我们要谨慎，因为我们删除这样的点，会导致标准差减小，可能又会有一些点符合Chauvenet准则，除非我们知道出现这种点的原因，不然我们不能随意的删除，通常我们希望修正或者重测。



### 4.非单值误差

前面我们在利用最大似然法时，我们假设所有的数据点都来自同一个parent distribution，但是在一些情形下，一些数据点可以比另一些数据点的测量精度更高一点，我们可以假定这些原始分布具有相同的平均值$\mu$，不同的标准差$\sigma$，这时，
$$
P\left(\mu^{\prime}\right)=\prod_{i=1}^{n}\left(\frac{1}{\sigma_{i} \sqrt{2 \pi}}\right) \exp \left[-\frac{1}{2} \sum\left(\frac{x_{i}-\mu^{\prime}}{\sigma_{i}}\right)^{2}\right]
$$
我们同样利用最大似然法，
$$
-\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} \mu^{\prime}} \sum\left(\frac{x_{i}-\mu^{\prime}}{\sigma_{i}}\right)^{2}=\sum\left(\frac{x_{i} - \mu^{\prime}}{\sigma_{i}{ }^{2}}\right)=0
$$
得到最可能的值$\mu^{\prime}$，它是各数据点的加权平均，而不像前面的平均值，
$$
\mu^{\prime}=\frac{\sum\left(x_{i} / \sigma_{i}{ }^{2}\right)}{\sum\left(1 / \sigma_{i}{ }^{2}\right)}
$$
我们接着来计算平均值的误差，利用误差传播公式，
$$
\frac{\partial \mu^{\prime}}{\partial x_{i}}=\frac{\partial}{\partial x_{i}} \frac{\sum\left(x_{i} / \sigma_{i}{ }^{2}\right)}{\sum 1 / \sigma_{i}{ }^{2}}=\frac{1 / \sigma_{i}{ }^{2}}{\sum\left(1 / \sigma_{i}{ }^{2}\right)}
$$

$$
\sigma_{\mu}{ }^{2}=\sum \frac{1 / \sigma_{i}{ }^{2}}{\left[\sum\left(1 / \sigma_{i}{ }^{2}\right)\right]^{2}}=\frac{1}{\sum\left(1 / \sigma_{i}{ }^{2}\right)}
$$

**相对误差：**

如果我们只知道$\sigma_{i}$的相对值，不知道其绝对值，我们就可以利用权重的概念来计算



### 5.统计涨落



### 6.卡方（$\chi^{2}$）检验

假设我现在测量一个量x得到了N个值（其中可能有相同的值），并且以同一观测值的出现率分组做直方图，如果出现了n个测量值$x_{j}$（这n个值都是不同的值，$j$从$1\rightarrow n$）。

$h(x_{j})$是$x_{j}$的出现率，如果我们知道在随机测量中，观测值$x_{j}$的出现概率为$P(x_{j})$，那么$h(x_{j})$的预期测量值为$y(x_{j})=NP(x_{j})$，但是$h(x_{j})$不一定等于$y(x_{j})$，它会有一定的分布。原则上它的分布应该由原始分布的参数给出，因为是泊松分布，$\mu_{j}=y(x_{j})$，$\sigma_{j}(h)=\sqrt{\mu_{j}}$，但是我们一般情况下不会知道原始分布的参数，所以我们只能用我们测量到的出现率$h(x_{j})$为中心，$\sigma_{j}(h)=\sqrt{h(x_j)}$为标准差。



因此我们定义$\chi^{2}$来表征观测到的出现率偏离预期出现率的dispersion(离散，弥散)，
$$
\chi^{2} \equiv \sum_{j=1}^{n} \frac{\left[h\left(x_{j}\right)-N P\left(x_{j}\right)\right]^{2}}{\sigma_{j}(h)^{2}}
$$
在多数实验中，我们并不知道$\sigma_{j}(h)$，但对于泊松分布，$\sigma_{j}(h)=\sqrt{N P\left(x_{j}\right)} \approx \sqrt{h\left(x_{j}\right)}$
$$
\chi^{2} \equiv \sum_{j=1}^{n} \frac{\left[h\left(x_{j}\right)-N P\left(x_{j}\right)\right]^{2}}{N P\left(x_{j}\right)} \approx \sum_{j=1}^{n} \frac{\left[h\left(x_{j}\right)-N P\left(x_{j}\right)\right]^{2}}{h\left(x_{j}\right)}
$$
如果观察到的出现频率和我预期的完全一直，$\chi^{2}=0$，但不是真实实验的结果。

我们来看一下分母和分子的意义，$\left[h\left(x_{j}\right)-N P\left(x_{j}\right)\right]^{2}$表征的是对观测值分布范围的度量，$\sigma_{j}(h)=\sqrt{N P\left(x_{j}\right)} \approx \sqrt{h\left(x_{j}\right)}$表征的是对预期的分布范围的度量。



$\chi^{2}$的真实预期值应该为，
$$
<\chi^{2}>=\nu=n-n_{c}
$$
$\nu$为自由度，$n$为取样出现率(就是能出现几种可能的测量)，$n_{c}$是由数据算得的概率函数$NP(x_{j})$具有的约束或者参数的数量。对于上面讨论的情况，$NP(x_{j})$独立于测量分布$h(x_{j})$